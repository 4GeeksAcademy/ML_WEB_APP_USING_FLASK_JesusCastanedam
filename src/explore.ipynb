{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Explore here"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "from flask import Flask, request, render_template, send_file\n",
                "from transformers import AutoModelForImageSegmentation\n",
                "import torch\n",
                "from torchvision import transforms\n",
                "from PIL import Image\n",
                "import io"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "app = Flask(__name__)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Detectar si hay GPU\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cargar el modelo solo una vez\n",
                "model = AutoModelForImageSegmentation.from_pretrained(\n",
                "    \"ZhengPeng7/BiRefNet\", trust_remote_code=True\n",
                ").to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Transformaciones\n",
                "transform_image = transforms.Compose([\n",
                "    transforms.Resize((1024, 1024)),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
                "])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
                "def index():\n",
                "    if request.method == \"POST\":\n",
                "        file = request.files[\"image\"]\n",
                "        image = Image.open(file.stream).convert(\"RGB\")\n",
                "        processed_image = remove_background(image)\n",
                "        \n",
                "        # Convertimos la imagen a bytes\n",
                "        img_io = io.BytesIO()\n",
                "        processed_image.save(img_io, format=\"PNG\")\n",
                "        img_io.seek(0)\n",
                "        \n",
                "        return send_file(img_io, mimetype='image/png')\n",
                "    \n",
                "    return '''\n",
                "        <h1>Background Remover</h1>\n",
                "        <form method=\"POST\" enctype=\"multipart/form-data\">\n",
                "            <input type=\"file\" name=\"image\">\n",
                "            <input type=\"submit\" value=\"Process\">\n",
                "        </form>\n",
                "    '''"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "def remove_background(image):\n",
                "    original_size = image.size\n",
                "    input_tensor = transform_image(image).unsqueeze(0).to(device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        preds = model(input_tensor)[-1].sigmoid().cpu()\n",
                "    \n",
                "    pred = preds[0].squeeze()\n",
                "    pred_pil = transforms.ToPILImage()(pred)\n",
                "    mask = pred_pil.resize(original_size)\n",
                "    \n",
                "    image.putalpha(mask)\n",
                "    return image"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        " * Serving Flask app '__main__'\n",
                        " * Debug mode: on\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
                        " * Running on http://127.0.0.1:5000\n",
                        "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
                        " * Restarting with stat\n",
                        "Traceback (most recent call last):\n",
                        "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
                        "  File \"<frozen runpy>\", line 88, in _run_code\n",
                        "  File \"/workspaces/ML_WEB_APP_USING_FLASK_JesusCastanedam/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
                        "    app.launch_new_instance()\n",
                        "  File \"/workspaces/ML_WEB_APP_USING_FLASK_JesusCastanedam/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1074, in launch_instance\n",
                        "    app.initialize(argv)\n",
                        "  File \"/workspaces/ML_WEB_APP_USING_FLASK_JesusCastanedam/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 118, in inner\n",
                        "    return method(app, *args, **kwargs)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/workspaces/ML_WEB_APP_USING_FLASK_JesusCastanedam/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 692, in initialize\n",
                        "    self.init_sockets()\n",
                        "  File \"/workspaces/ML_WEB_APP_USING_FLASK_JesusCastanedam/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 331, in init_sockets\n",
                        "    self.shell_port = self._bind_socket(self.shell_socket, self.shell_port)\n",
                        "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/workspaces/ML_WEB_APP_USING_FLASK_JesusCastanedam/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 253, in _bind_socket\n",
                        "    return self._try_bind_socket(s, port)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/workspaces/ML_WEB_APP_USING_FLASK_JesusCastanedam/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 229, in _try_bind_socket\n",
                        "    s.bind(\"tcp://%s:%i\" % (self.ip, port))\n",
                        "  File \"/workspaces/ML_WEB_APP_USING_FLASK_JesusCastanedam/.venv/lib/python3.11/site-packages/zmq/sugar/socket.py\", line 320, in bind\n",
                        "    super().bind(addr)\n",
                        "  File \"_zmq.py\", line 942, in zmq.backend.cython._zmq.Socket.bind\n",
                        "  File \"_zmq.py\", line 180, in zmq.backend.cython._zmq._check_rc\n",
                        "zmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002')\n"
                    ]
                },
                {
                    "ename": "SystemExit",
                    "evalue": "1",
                    "output_type": "error",
                    "traceback": [
                        "An exception has occurred, use %tb to see the full traceback.\n",
                        "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/workspaces/ML_WEB_APP_USING_FLASK_JesusCastanedam/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3678: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
                        "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
                    ]
                },
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
                        "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
                        "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
                        "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
                    ]
                }
            ],
            "source": [
                "if __name__ == \"__main__\":\n",
                "    app.run(debug=True)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
